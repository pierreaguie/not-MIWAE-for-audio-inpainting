{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(nn.Linear(1000,50), nn.ReLU(), nn.Linear(50,10))\n",
    "decoder = nn.Sequential(nn.Linear(10,50), nn.ReLU(), nn.Linear(50,1000))\n",
    "missing_model = nn.Sequential(nn.Linear(1000, 1000), nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, Bernoulli, Independent\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class notMIWAE(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, missing_model, encoder_input_dim, encoder_output_dim, decoder_output_dim, latent_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.missing_model = missing_model\n",
    "\n",
    "        self.encoder_input_dim = encoder_input_dim\n",
    "        self.encoder_output_dim = encoder_output_dim\n",
    "        self.decoder_output_dim = decoder_output_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.q_mu = nn.Linear(encoder_output_dim, latent_dim)\n",
    "        self.q_logvar = nn.Linear(encoder_output_dim, latent_dim)\n",
    "        self.p_mu = nn.Linear(decoder_output_dim, encoder_input_dim)\n",
    "        self.p_logvar = nn.Linear(decoder_output_dim, encoder_input_dim)\n",
    "\n",
    "        # Prior distribution\n",
    "        self.p_z = Independent(Normal(torch.zeros(1), torch.ones(1)),1)\n",
    "\n",
    "\n",
    "    def forward(self, x : torch.Tensor, s : torch.Tensor, K : int = 1) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the not-MIWAE loss\n",
    "        \n",
    "        Inputs:\n",
    "        ------\n",
    "        - x: Tensor of shape (batch_size, input_dim)\n",
    "        - s: Tensor of shape (batch_size, input_dim) with 1s where x is observed and 0s where x is missing\n",
    "        - K: Number of samples to draw from the posterior (default = 1)\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "        - loss: Tensor of shape (1) with the not-MIWAE loss\n",
    "        \"\"\"\n",
    "        # s[i,j] = 1 iff x[i,j] is observed. If x[i,j] is missing, pad with 0.\n",
    "        x_observed = s * x                                                # Size (bs,input_dim)\n",
    "\n",
    "        # Encoder: q(z|x_observed)\n",
    "        h = self.encoder(x_observed)\n",
    "        mu_z = self.q_mu(h)\n",
    "        logvar_z = self.q_logvar(h)\n",
    "\n",
    "        # Sampling z from q(z|x_observed) K times\n",
    "        q_z_given_x = Independent(Normal(loc = mu_z, scale = torch.exp(0.5 * logvar_z)), 1)\n",
    "        z = q_z_given_x.rsample([K])                                      # Size (K,bs,latent_dim)\n",
    "        \n",
    "        # Decoder: p(x|z_k) for all k = 1, ..., K\n",
    "        h = self.decoder(z)\n",
    "        mu_x = self.p_mu(h)\n",
    "        logvar_x = self.p_logvar(h)\n",
    "        p_x_given_z = Independent(Normal(mu_x, torch.exp(0.5 * logvar_x)),1)\n",
    "\n",
    "        # Sample missing data\n",
    "        x_missing = p_x_given_z.rsample() * (1 - s)\n",
    "        x_imputed = x_observed + x_missing\n",
    "\n",
    "        # Missing model: p(s|x_observed, x_missing)\n",
    "        p_s_given_x = Independent(Bernoulli(self.missing_model(x_imputed)),1)\n",
    "\n",
    "        # Log probabilities:\n",
    "        x_observed_k = torch.Tensor.repeat(x_observed, [K,1,1])            # Size (K,bs,input_dim)\n",
    "        s_k = torch.Tensor.repeat(s, [K,1,1])                              \n",
    "        \n",
    "        log_p_x_given_z = p_x_given_z.log_prob(x_observed_k)               # Size (K, bs)\n",
    "        log_q_z_given_x = q_z_given_x.log_prob(z)\n",
    "        log_p_s_given_x = p_s_given_x.log_prob(s_k)\n",
    "        log_p_z = self.p_z.log_prob(z)\n",
    "\n",
    "        loss = -torch.mean(torch.logsumexp(log_p_x_given_z + log_p_s_given_x - log_q_z_given_x + log_p_z - np.log(K), dim = 0))\n",
    "     \n",
    "        return loss\n",
    "\n",
    "    def impute(self, x : torch.Tensor, s : torch.Tensor, K : int = 1) -> torch.Tensor:\n",
    "            \"\"\" \n",
    "            Imputes missing values in x using the trained model\n",
    "            \n",
    "            Inputs:\n",
    "            ------\n",
    "            - x: Tensor of shape (batch_size, input_dim)\n",
    "            - s: Tensor of shape (batch_size, input_dim) with 1s where x is observed and 0s where x is missing\n",
    "            - K: Number of samples to draw from the posterior (default = 1)\n",
    "\n",
    "            Outputs:\n",
    "            -------\n",
    "            - x_imputed: Tensor of shape (batch_size, input_dim) with the imputed values and 0s where x is observed\n",
    "            \"\"\"\n",
    "            # s[i,j] = 1 iff x[i,j] is observed. If x[i,j] is missing, pad with 0.\n",
    "            x_observed = s * x                                                # Size (bs,input_dim)\n",
    "\n",
    "            # Encoder: q(z|x_observed)\n",
    "            h = self.encoder(x_observed)\n",
    "            mu_z = self.q_mu(h)\n",
    "            logvar_z = self.q_logvar(h)\n",
    "\n",
    "            # Sampling z from q(z|x_observed) K times\n",
    "            q_z_given_x = Independent(Normal(loc = mu_z, scale = torch.exp(0.5 * logvar_z)), 1)\n",
    "            z = q_z_given_x.rsample([K])                                      # Size (K,bs,latent_dim)\n",
    "            \n",
    "            # Decoder: p(x|z_k) for all k = 1, ..., K\n",
    "            h = self.decoder(z)\n",
    "            mu_x = self.p_mu(h)\n",
    "            logvar_x = self.p_logvar(h)\n",
    "            p_x_given_z = Independent(Normal(mu_x, torch.exp(0.5 * logvar_x)),1)\n",
    "\n",
    "            # Sample missing data\n",
    "            x_missing = p_x_given_z.rsample() * (1 - s)\n",
    "            x_imputed = x_observed + x_missing\n",
    "\n",
    "            # Missing model: p(s|x_observed, x_missing)\n",
    "            p_s_given_x = Independent(Bernoulli(self.missing_model(x_imputed)),1)\n",
    "\n",
    "            # Importance weights:\n",
    "            x_observed_k = torch.Tensor.repeat(x_observed, [K,1,1])            # Size (K,bs,input_dim)\n",
    "            s_k = torch.Tensor.repeat(s, [K,1,1])                              \n",
    "            \n",
    "            log_p_x_given_z = p_x_given_z.log_prob(x_observed_k)               # Size (K, bs)\n",
    "            log_q_z_given_x = q_z_given_x.log_prob(z)\n",
    "            log_p_s_given_x = p_s_given_x.log_prob(s_k)\n",
    "            log_p_z = self.p_z.log_prob(z)\n",
    "\n",
    "            imp_weights = F.softmax(log_p_x_given_z + log_p_s_given_x - log_q_z_given_x + log_p_z - np.log(K), dim = 0)\n",
    "\n",
    "            return torch.einsum('ki,kij->ij', imp_weights, x_missing)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = notMIWAE(encoder, decoder, missing_model, 1000, 10, 1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64, 1000)\n",
    "s = torch.randint(0, 2, (64, 1000)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6836,  0.0000, -0.0872,  ..., -0.3183, -1.2457,  0.0909],\n",
       "        [ 0.0000,  0.4767,  0.0000,  ..., -0.5510, -0.7413,  0.5663],\n",
       "        [ 0.0000,  0.0000, -0.8638,  ..., -1.2282,  0.0000,  0.8220],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.2372,  0.7566, -0.5400],\n",
       "        [ 0.0000, -0.9929,  1.1133,  ...,  0.0000,  0.6700,  0.0000],\n",
       "        [-0.5146,  0.0000, -0.3997,  ...,  0.0000,  0.1830,  0.0000]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.impute(x, s, K = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
